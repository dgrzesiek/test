{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Chapter 7 of our book covers Ensemble Learning and Random Forests. All the methods described in the chapter are fairly important to know but this notebook focuses on XGBoost.\n",
    "\n",
    "### Install XGBoost\n",
    "The first thing you need to do is install the XGBoost Library. You can install it for Anaconda using\n",
    "\n",
    "    conda install -c conda-forge xgboost\n",
    "    \n",
    "If that doesn't work use Google to search for your particular installation specs. \n",
    "\n",
    "## Example using the Iris Data Set\n",
    "\n",
    "### Load the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = pd.read_csv('https://raw.githubusercontent.com/zacharski/machine-learning/master/data/iris.csv')\n",
    "\n",
    "iris_train, iris_test = train_test_split(iris, test_size = 0.2)\n",
    "train_X = iris_train[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
    "train_y = iris_train['Class']\n",
    "test_X = iris_test[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]\n",
    "test_y = iris_test['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an instance of the XGBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "model = XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The simplist example: fitting model to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skool/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris_predictions = model.predict(test_X)\n",
    "accuracy_score(test_y, iris_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a deprecation warning about an empty array it is caused by numpy which states:\n",
    "\n",
    "> The long and short is that truth-testing on empty arrays is dangerous, misleading, and not in any way useful, and should be deprecated.\n",
    "\n",
    "Unfortunately, this is done in sklearn version 19.1. sklearn 19.2 fixes this but it is not yet (at the time of making this notebook) available in Anaconda\n",
    "\n",
    "# The task - The Adult Dataset\n",
    "\n",
    "For this project we are going to use the [Adult Dataset](http://archive.ics.uci.edu/ml/datasets/Adult). The webpage describes the problem. We are trying to predict wether someone makes more that $50,000 year based on a number of features. The data folder contains both training data `adult.data` and test data `adult.test`. \n",
    "\n",
    "## Prepare the data. \n",
    "1. Values that are missing are represented by a `?`. You should remove rows containing missing data.\n",
    "2. In the training data the values of the column we are trying to predict (`wage_class`) are `<=50K` and `>50K`. Unfortunately, in the test data the values are `<=50K.` and `>50K.` (note the periods). You need to alter this so they are the same. You can use the Pandas DataFrame `.replace` method.\n",
    "\n",
    "## Finding the best hyperpameters\n",
    "We are interested in finding the best combination of hyperparameters:\n",
    "* max_depth with values 3, 5, and 7\n",
    "* min_child_weight with values 1, 3, 5\n",
    "\n",
    "The hyperparameters of the XGBClassifier we are not adjusting include:\n",
    "       'learning_rate': 0.1, \n",
    "       'n_estimators': 1000, \n",
    "       'seed':0, \n",
    "       'subsample': 0.8, \n",
    "       'colsample_bytree': 0.8, \n",
    "       'objective': 'binary:logistic'\n",
    "       \n",
    "Finally, the hyperparameters of the Grid Search include\n",
    "\n",
    "    scoring = 'accuracy', \n",
    "    cv = 5, \n",
    "    n_jobs = -1)\n",
    "    \n",
    "#### Here is what I got\n",
    "\n",
    "    optimized_GBM.best_params_\n",
    "    {'max_depth': 3, 'min_child_weight': 1}\n",
    "    \n",
    "    cvres = optimized_GBM.cv_results_\n",
    "    for mean_score, params in zip (cvres['mean_test_score'], cvres['params']):\n",
    "         print(np.sqrt(mean_score), params)\n",
    "         \n",
    "    0.9320100122602681 {'max_depth': 3, 'min_child_weight': 1}\n",
    "    0.9319922256399311 {'max_depth': 3, 'min_child_weight': 3}\n",
    "    0.9313872784451372 {'max_depth': 3, 'min_child_weight': 5}\n",
    "    0.92858873422616 {'max_depth': 5, 'min_child_weight': 1}\n",
    "    0.9288386283598029 {'max_depth': 5, 'min_child_weight': 3}\n",
    "    0.9280172949509803 {'max_depth': 5, 'min_child_weight': 5}\n",
    "    0.9262113772874686 {'max_depth': 7, 'min_child_weight': 1}\n",
    "    0.9250652086256056 {'max_depth': 7, 'min_child_weight': 3}\n",
    "    0.926372443522455 {'max_depth': 7, 'min_child_weight': 5}\n",
    "    \n",
    "## Finally run the best model on the test data\n",
    "\n",
    "    model = optimized_GBM.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
